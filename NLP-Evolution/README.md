## Мини - нейросеть
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

$L(y, p) = - y \log p - (1 - y) \log (1 - p)$

Это пример встроенной формулы: $f'(x) = \frac{dy}{dx}$.

А вот блочная формула:

$$
f'(x) = \frac{dy}{dx}
$$

## Обучаясь понимать контекст
### GloVe и Word2Vec
Ранние методы, такие как **Glove** и **Word2Vec**, были основаны на идее, что значение слово можно понять через его окружение - то есть через слова, которые часто встречаются рядом с ним. Это называется **распределённым представлением**.
* **Word2Vec**: Здесь для каждого слова строится два вектора: один для случая, когда слово находится в центре контекста, а другой - когда оно является соседним. Модель учится предсказывать соседний слова, что позволяет ей улавливать семантические связи между словами.
* **GloVe**: Этот метод также использует идею распределения слов, но работает с глобальной статистикой текста, например с частотой совместного появления слов.
### Проблема учёта локального контекста
Хотя Word2Vec и GloVe хорошо работают с локальным контекстом, они не учитывают, что значение слова может меняться в зависимости от более широкого окружения.
### Рекуррентные нейронные сети (RNN)
Нейросети стали мощным инструментом для работы с текстом, потому что они способны выделять сложные паттерны взаимодействия данных. Настоящей рабочей лошадкой, на которой впоследствии строились сложные архитектуры, стали рекуррентные нейронные сети. 
* **Идея RNN проста:** у сети есть внутреннее состояние, которое обновляется при обработке каждого слова в последовательности. Это позволяет сети помнить предыдущие слова и учитывать их при анализе текущего слова.
* **Как это работает:** RNN обрабатывает текст по одному слову за раз. На каждом шаге она обновляет своё внутренее состояние, которое зависит от текущего слова и предыдущего состояния. Это позволяет сети учитывать контекст из нескольких слов.
* **Проблема RNN:** у RNN есть серьёзный недостаток - **затухание градиента**. Это означает, что при обработке длинных текстов сеть забывает информацию из начала последовательности.
### Механизм внимания (Attention): решение проблемы глобального контекста
* **Идея Attention:** Вместо того чтобы усреднять или суммировать информацию из всех слов (как это делается в RNN), механизм внимания позволяет сети смотреть на все слова в тексте и решать, какие из них наиболее важны для текущего слова.
1. Какдый токен создаёт запрос $Q$, ключ $K$ и значение $V$
2. Схожесть между токенами оценивается через скалярное произведение $Q \times K^T$
* **Как это работает:**
1. Для каждого слова вычисляется его важность (вес) относительно других слов в тексте
2. Эти слова нормирутся с помощью функции Softmax, чтобы они суммировались в единицу
3. Затем каждый вектор слова умножается на его вес, а результаты - суммируются. Это позволяет сети выделять наиболее важные слова для понимания контекста
### Трансформеры и Self-Attention: революция в NLP
* Базовая, верхнеуровневая структура
* Трансформеры - это архитектура, которая полностью построена на механизме внимания. В отличие от RNN, трансформеры не обрабатывают текст последовательно, а анализирует все слова одновременно с помощью механизма **Self-Attention.**
* **Self-Attention -** это механизм, который позволяет каждому слову взаимодействовать с другими словами в тексте. Каждое слово получает новый вектор, который учитывает его отношение со всеми остальными словами.
* Трансформеры быстрее обучаются, так как могут обрабатывать все элементы последовательности одновременно (в отличие от RNN, которые обрабатывают последовательности поэлементон).
### Multi-Head Attention
### Параллелизм в генеративных моделях
### Как мы отбираем признаки, что можем перенести в другой формат, а что - нет

## Краткая справка по нейросетям
### Накопление градиента
### Batch Normalization
Батч-нормализация - это техника, которая помогает стабилизировать и ускорить обучение нейронных сетей. Её основная идея - нормализовать данные, которые проходят через слои нейронной сети, чтобы они имели среднее значение **нуль**, а стандартное отклонение - **единице.**
* **Как это работает:**
1. **Батч:** Во время обучения нейронная сеть обрабатывает данные не по одному примеру, а небольшими группами - батчами.
2. **Нормализация:** Для каждого батча вычисляется среднее значение и стандартное отклонение по всем примерам в батче. Затем данные нормализуются так, чтобы их распределение стало стандартным.
3. **Масштабирование и сдвиг:** После нормализации данные масштабируются и сдвигаются с помощью двух обучаемых параметров - $\gamma$ и $\beta$. Это позволяет сети сохранить гибкость и при необходимости отменить нормализацию, если это полезно для задачи.
### Layer Normalization
### Residual Connection (Остаточное соединение)

## База
### Абстрактный класс

### Фабрика классов

### Singleton (Синглтон)





