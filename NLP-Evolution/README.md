## Мини - нейросеть
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

$P(y = 1 | x) = \sigma(z) = \frac{1}{1 + e^{-z}}$ и $P(y = 0 | x) = 1 - \sigma(z) = \frac{e^{-z}}{1 + e^{-z}}$

Это пример встроенной формулы: $f'(x) = \frac{dy}{dx}$.

А вот блочная формула:

$$
f'(x) = \frac{dy}{dx}
$$

## Обучаясь понимать контекст ...
### GloVe
### Word2Vec
Идея - распределение вероятностей соседних слов
Для каждого слова строится два вектора - слово в центре окна
### Проблема учёта локального контекста
Каждому уникальному элементу сопостовляется идентификатор.
Таблица представлений (таблица эмбеддингов) хранится в нейросети.
* Следующий шаг - учёт локального контекста. Мы хотим привнести в вектор для каждого слова информацию о том, какие слова идут перед ним, а какие - после него
* Получаем тензор другой формы
* Теперь добавляем глобальный контекст агрегацией (pooling) - весть текст сжимается в вектор
### Задача моделирования языка
Идея - предсказать следующее слово по контексту
Гипотеза - смысл слова в значительной мере описывается контекстом
### Нейросети
Смысл использования нейросетей в том, что они могу выделить сложные паттерны взаимодействия данных
Рабочая лошадка - `рекуррентная нейронная сеть`
### Рекуррентная нейронная сеть
Идея: внутренне состояние, которое мы обновляем при прохождении входной последовательности
Значение состояние зависит от предыдущего значения
Но нейронные сети плохо работают с глобальным контекстом - затухание градиента
### Как получить один вектор, представляющий один текст? Или как получить глобальный контекст?
Рассмотрим матрица эмбеддингов размером Число слов X Размер эмбеддингов
На выходе блоков агрегации получается один вектор
Pooling может быть как Усреднением, так и Максимальным
Но придумали агрегацию получше - механизм внимания.
### Механизм внимания - attention
Механизм внимания: разделение операции оценки значимости вектора и получение информации из него
* Матрица размером Число слов X Размер эмбеддингов проходит через свёрточную нейронную сеть
* Полученный набор logits нормируется через Softmax
* Полученный вектор умножается на входную матрицу
* Выходной вектор равен размеру эмбеддинга
Нейросети решают задачу оценки значимости каждого элемента текста
С ростом длины последовательности внимание не работает хуже, а pooling с усреднением приводит к затуханию градиента
Возможность интерпретации
## Трансформеры и self-attention




