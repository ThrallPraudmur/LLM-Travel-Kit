## Мини - нейросеть
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

$L(y, p) = - y \log p - (1 - y) \log (1 - p)$

Это пример встроенной формулы: $f'(x) = \frac{dy}{dx}$.

А вот блочная формула:

$$
f'(x) = \frac{dy}{dx}
$$

## Обучаясь понимать контекст
### GloVe и Word2Vec
Ранние методы, такие как **Glove** и **Word2Vec**, были основаны на идее, что значение слово можно понять через его окружение - то есть через слова, которые часто встречаются рядом с ним. Это называется **распределённым представлением**.
* **Word2Vec**: Здесь для каждого слова строится два вектора: один для случая, когда слово находится в центре контекста, а другой - когда оно является соседним. Модель учится предсказывать соседний слова, что позволяет ей улавливать семантические связи между словами.
* **GloVe**: Этот метод также использует идею распределения слов, но работает с глобальной статистикой текста, например с частотой совместного появления слов.
### Проблема учёта локального контекста
Хотя Word2Vec и GloVe хорошо работают с локальным контекстом, они не учитывают, что значение слова может меняться в зависимости от более широкого окружения.
### Рекуррентные нейронные сети (RNN)
Нейросети стали мощным инструментом для работы с текстом, потому что они способны выделять сложные паттерны взаимодействия данных. Настоящей рабочей лошадкой, на которой впоследствии строились сложные архитектуры, стали рекуррентные нейронные сети. 
* **Идея RNN проста:** у сети есть внутреннее состояние, которое обновляется при обработке каждого слова в последовательности. Это позволяет сети помнить предыдущие слова и учитывать их при анализе текущего слова.
* **Как это работает:** RNN обрабатывает текст по одному слову за раз. На каждом шаге она обновляет своё внутренее состояние, которое зависит от текущего слова и предыдущего состояния. Это позволяет сети учитывать контекст из нескольких слов.
* **Проблема RNN:** у RNN есть серьёзный недостаток - **затухание градиента**. Это означает, что при обработке длинных текстов сеть забывает информацию из начала последовательности.
### Механизм внимания (Attention): решение проблемы глобального контекста
* **Идея Attention:** Вместо того чтобы усреднять или суммировать информацию из всех слов (как это делается в RNN), механизм внимания позволяет сети смотреть на все слова в тексте и решать, какие из них наиболее важны для текущего слова.
* **Как это работает:**
  1.
  2.
## Трансформеры и self-attention





