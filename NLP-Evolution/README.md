## Мини - нейросеть
`Логистическая регрессия` - это метод классификации, который использует линейную комбинацию признаков, котору затем преобразует через сигмоидную функцию:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

Минизировать отрицательный логарифм -> кроссэнтропия:

$$
L(y, p) = - y * \log p - (1 - y) * \log (1 - p)
$$
1. При $y = 1$ вторая часть зануляется, штрафуем за предсказание слишком маленькой вероятности
2. При $y = 0$ первая часть зануляется, штрафуем за предсказание слишком большой вероятности

`Стохастический спуск` и `нормальное уравнение`.

## Обучаясь понимать контекст
### GloVe и Word2Vec
Ранние методы, такие как `Glove` и `Word2Vec`, были основаны на идее, что значение слово можно понять через его окружение - то есть через слова, которые часто встречаются рядом с ним. Это называется `распределённым представлением`.
* `Word2Vec`: Здесь для каждого слова строится два вектора: один для случая, когда слово находится в центре контекста, а другой - когда оно является соседним. Модель учится предсказывать соседний слова, что позволяет ей улавливать семантические связи между словами.
* `GloVe`: Этот метод также использует идею распределения слов, но работает с глобальной статистикой текста, например с частотой совместного появления слов.
### Проблема учёта локального контекста
Хотя Word2Vec и GloVe хорошо работают с локальным контекстом, они не учитывают, что значение слова может меняться в зависимости от более широкого окружения. Рекуррентная нейронная сеть пытается решить этту проблему, обрабатывая текст слово за словом, чтобы понять последовательность.
### Рекуррентные нейронные сети (RNN)
Нейросети стали мощным инструментом для работы с текстом, потому что они способны выделять сложные паттерны взаимодействия данных. Настоящей рабочей лошадкой, на которой впоследствии строились сложные архитектуры, стали рекуррентные нейронные сети. 
* `Идея RNN проста`: у сети есть внутреннее состояние, которое обновляется при обработке каждого слова в последовательности. Это позволяет сети помнить предыдущие слова и учитывать их при анализе текущего слова.
* `Как это работает`: RNN обрабатывает текст по одному слову за раз. На каждом шаге она обновляет своё внутренее состояние, которое зависит от текущего слова и предыдущего состояния. Это позволяет сети учитывать контекст из нескольких слов.
* `Проблема RNN`: у RNN есть серьёзный недостаток - `затухание градиента`. Это означает, что при обработке длинных текстов сеть забывает информацию из начала последовательности.
### Batch Normalization
Батч-нормализация - это техника, которая помогает стабилизировать и ускорить обучение нейронных сетей. Её основная идея - нормализовать данные, которые проходят через слои нейронной сети, чтобы они имели среднее значение `нуль`, а стандартное отклонение - `единице`.
* **Как это работает:**
1. `Батч`: Во время обучения нейронная сеть обрабатывает данные не по одному примеру, а небольшими группами - батчами.
2. `Нормализация`: Для каждого батча вычисляется среднее значение и стандартное отклонение по всем примерам в батче. Затем данные нормализуются так, чтобы их распределение стало стандартным.
3. `Масштабирование и сдвиг`: После нормализации данные масштабируются и сдвигаются с помощью двух обучаемых параметров - $\gamma$ и $\beta$. Это позволяет сети сохранить гибкость и при необходимости отменить нормализацию, если это полезно для задачи.
### Механизм внимания (Attention): решение проблемы глобального контекста
* `Идея Attention`: Вместо того чтобы усреднять или суммировать информацию из всех слов (как это делается в RNN), механизм внимания позволяет сети смотреть на все слова в тексте и решать, какие из них наиболее важны для текущего слова.
1. Какдый токен создаёт запрос $Q$, ключ $K$ и значение $V$
2. Схожесть между токенами оценивается через скалярное произведение $Q \times K^T$
* `Как это работает`:
1. Для каждого слова вычисляется его важность (вес) относительно других слов в тексте
2. Эти слова нормирутся с помощью функции Softmax, чтобы они суммировались в единицу
3. Затем каждый вектор слова умножается на его вес, а результаты - суммируются. Это позволяет сети выделять наиболее важные слова для понимания контекста
### Трансформеры и Self-Attention: революция в NLP
* Трансформеры - это архитектура, которая полностью построена на механизме внимания. В отличие от RNN, трансформеры не обрабатывают текст последовательно, а анализирует все слова одновременно с помощью механизма `Self-Attention`.
* `Self-Attention` - это механизм, который позволяет каждому слову взаимодействовать с другими словами в тексте. Каждое слово получает новый вектор, который учитывает его отношение со всеми остальными словами.
* Трансформеры быстрее обучаются, так как могут обрабатывать все элементы последовательности одновременно (в отличие от RNN, которые обрабатывают последовательности поэлементон).
* `Энкодер` состоит из нескольких идентичных слоёв, каждый из которых включает `Multi-Head Attention`, `Feed-Forward Network`, `Layer Normalization` и `Residual Connections`
* `Декодер` аналогичен `Энкодеру`, но с дополнительным механизмом внимания над выходом энкодера, а также с `Masked Multi-Head Attention`, для предотвращения подглядывания в будущие токены.
* `BERT` использует только `Энкодер` для извлечения контекстных представлений текста и видит весь контекст одновременно: как предыдущие, так и последующие токены.
### Multi-Head Attention
### Layer Normalization
### Residual Connection (Остаточное соединение)
### Параллелизм в генеративных моделях
### Как мы отбираем признаки, что можем перенести в другой формат, а что - нет


### Накопление градиента
### Batch Normalization
Батч-нормализация - это техника, которая помогает стабилизировать и ускорить обучение нейронных сетей. Её основная идея - нормализовать данные, которые проходят через слои нейронной сети, чтобы они имели среднее значение **нуль**, а стандартное отклонение - **единице.**
* `Как это работает`:
1. `Батч`: Во время обучения нейронная сеть обрабатывает данные не по одному примеру, а небольшими группами - батчами.
2. `Нормализация`: Для каждого батча вычисляется среднее значение и стандартное отклонение по всем примерам в батче. Затем данные нормализуются так, чтобы их распределение стало стандартным.
3. `Масштабирование и сдвиг`: После нормализации данные масштабируются и сдвигаются с помощью двух обучаемых параметров - $\gamma$ и $\beta$. Это позволяет сети сохранить гибкость и при необходимости отменить нормализацию, если это полезно для задачи.

## База
### Абстрактный класс
1. Абстрактный класс гарантирует, что все классы-наследники будут иметь определённые методы.
2. Абстрактный класс - это как шаблон для других классов. Он описывает, какие методы и свойства должны быть у классов, которые от него наследуются, но сам по себе не предназначен для создания объектов.
```python
from abc import ABC, abstractmethod

# Абстрактный класс
class Animal(ABC):
    @abstractmethod
    def speak(self):
        pass
```
### Фабрика классов
* Фабрика классов - это способ создания объектов без явного указания их класса. Это как фабрика, которая производит объекты в зависимости от того, что ты ей скажешь.
* Фабрика классов нужна для того, чтобы упростить создание объектов и скрыть сложность их создания.
### Singleton (Синглтон)
* **Синглтон -** это класс, который гарантирует, что в программе будет существовать только один объект этого класса. Это полезно, когда нужно, чтобы все части программы использовали один и тот же экземпляр. Например, если у тебя будет несколько подключений к базе данных, это может вызвать конфликты или перегрузку системы.





