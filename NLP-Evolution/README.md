## Мини - нейросеть
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

$L(y, p) = - y \log p - (1 - y) \log (1 - p)$

Это пример встроенной формулы: $f'(x) = \frac{dy}{dx}$.

А вот блочная формула:

$$
f'(x) = \frac{dy}{dx}
$$

## Обучаясь понимать контекст ...
### GloVe и Word2Vec
Ранние методы, такие как **Glove** и **Word2Vec**, были основаны на идее, что значение слово можно понять через его окружение - то есть через слова, которые часто встречаются рядом с ним. Это называется **распределённым представлением**.
* **Word2Vec**: Здесь для каждого слова строится два вектора: один для случая, когда слово находится в центре контекста, а другой - когда оно является соседним. Модель учится предсказывать соседний слова, что позволяет ей улавливать семантические связи между словами.
* **GloVe**: Этот метод также использует идею распределения слов, но работает с глобальной статистикой текста, например с частотой совместного появления слов.
### Проблема учёта локального контекста
Хотя Word2Vec и GloVe хорошо работают с локальным контекстом, они не учитывают, что значение слова может меняться в зависимости от более широкого окружения.
### Рекуррентные нейронные сети (RNN)
Нейросети стали мощным инструментом для работы с текстом, потому что они способны выделять сложные паттерны взаимодействия данных. Настоящей рабочей лошадкой, на которой строится большинство современных архитектур, стали рекуррентные нейронные сети.

Идея: внутренне состояние, которое мы обновляем при прохождении входной последовательности
Значение состояние зависит от предыдущего значения
Но нейронные сети плохо работают с глобальным контекстом - затухание градиента
### Как получить один вектор, представляющий один текст? Или как получить глобальный контекст?
Рассмотрим матрица эмбеддингов размером Число слов X Размер эмбеддингов
На выходе блоков агрегации получается один вектор
Pooling может быть как Усреднением, так и Максимальным
Но придумали агрегацию получше - механизм внимания.
### Механизм внимания - attention
Механизм внимания: разделение операции оценки значимости вектора и получение информации из него
* Матрица размером Число слов X Размер эмбеддингов проходит через свёрточную нейронную сеть
* Полученный набор logits нормируется через Softmax
* Полученный вектор умножается на входную матрицу
* Выходной вектор равен размеру эмбеддинга
Нейросети решают задачу оценки значимости каждого элемента текста
С ростом длины последовательности внимание не работает хуже, а pooling с усреднением приводит к затуханию градиента
Возможность интерпретации
## Трансформеры и self-attention




